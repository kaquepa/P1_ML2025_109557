# Bank Marketing Campaign - Machine Learning Classification Report

**Author:** [Your Name]  
**Date:** November 7, 2025  
**Course:** Machine Learning  
**Dataset:** Bank Marketing Dataset (UCI Machine Learning Repository)

---

## Executive Summary

This report presents a comprehensive machine learning solution for predicting term deposit subscriptions in bank marketing campaigns. Three classification models were developed and evaluated: Logistic Regression, Random Forest, and XGBoost. The best model achieved an AUC-ROC of 0.7963 with 67.56% recall on the positive class, enabling the bank to identify high-potential customers and optimize marketing resources.

---

## 1. Introduction & Problem Motivation

### 1.1 Business Context

Direct marketing campaigns are costly and resource-intensive for financial institutions. Banks contact thousands of potential clients via phone calls to promote term deposits, but conversion rates are typically very low (around 11% in this dataset). This creates significant challenges:

- **High operational costs**: Call center operations require substantial human and financial resources
- **Customer fatigue**: Excessive contacts can damage customer relationships
- **Low efficiency**: Contacting all customers indiscriminately wastes resources on unlikely prospects

### 1.2 Problem Statement

**Machine Learning Problem Definition:**

- **Type**: Binary Classification (Supervised Learning)
- **Target Variable**: `y` - Client subscribed to term deposit (yes/no)
- **Input Features**: 20 attributes covering:
  - Client demographics (age, job, marital status, education)
  - Financial status (housing loan, personal loan, credit default)
  - Campaign information (contact type, month, day of week)
  - Previous campaign outcomes
  - Economic indicators (employment rate, consumer confidence, etc.)
- **Dataset Size**: 41,188 instances
- **Output**: Binary prediction (0 = No subscription, 1 = Subscription)

### 1.3 Project Objective

Develop a predictive model that:
1. Identifies clients with high propensity to subscribe
2. Maximizes recall while maintaining acceptable precision
3. Enables targeted marketing to reduce costs by 40-60%
4. Improves conversion rates by 2-3x

### 1.4 Problem Complexity

**Complexity Level: Medium-High**

This problem presents several challenges that justify its complexity:

1. **Severe Class Imbalance**: 88.9% negative class vs 11.1% positive class
2. **High Dimensionality**: 20 input features with mixed data types
3. **Unknown Values**: Significant missing data encoded as "unknown"
4. **Temporal Dependencies**: Economic indicators and campaign timing affect outcomes
5. **Business Constraints**: High recall is critical (missing potential customers is costly)
6. **Non-linear Relationships**: Complex interactions between demographic and economic factors

---

## 2. Data Description

### 2.1 Dataset Characteristics

| Characteristic | Value |
|---------------|-------|
| **Total Instances** | 41,188 |
| **Training Samples** | 32,950 (80%) |
| **Test Samples** | 8,238 (20%) |
| **Total Features** | 20 input + 1 target |
| **Numerical Features** | 10 |
| **Categorical Features** | 10 |
| **Missing Values** | 0 (encoded as "unknown") |
| **Memory Usage** | ~8.5 MB |
| **Duplicates** | 0 |

### 2.2 Target Variable Distribution

**Severe Class Imbalance Detected:**

| Class | Count | Percentage | Imbalance Ratio |
|-------|-------|------------|-----------------|
| No (0) | 36,548 | 88.9% | 7.98:1 |
| Yes (1) | 4,640 | 11.1% | - |

This severe imbalance requires special handling (SMOTE) to prevent model bias toward the majority class.

### 2.3 Feature Categories

#### Client Demographics
- `age`: Client age (numeric, 17-98 years)
- `job`: Type of job (12 categories: admin, blue-collar, entrepreneur, etc.)
- `marital`: Marital status (married, single, divorced, unknown)
- `education`: Education level (basic.4y, basic.6y, basic.9y, high.school, university, etc.)

#### Financial Information
- `default`: Has credit in default? (yes/no/unknown)
- `housing`: Has housing loan? (yes/no/unknown)
- `loan`: Has personal loan? (yes/no/unknown)

#### Campaign Contact Information
- `contact`: Contact communication type (cellular, telephone)
- `month`: Last contact month (jan to dec)
- `day_of_week`: Last contact day (mon to fri)
- `duration`: Last contact duration in seconds **(removed - not available before contact)**

#### Previous Campaign
- `campaign`: Number of contacts during this campaign (1-56)
- `pdays`: Days since last contact from previous campaign (999 = not contacted)
- `previous`: Number of contacts before this campaign (0-7)
- `poutcome`: Outcome of previous campaign (failure, nonexistent, success)

#### Economic Context Indicators
- `emp.var.rate`: Employment variation rate (quarterly, -3.4 to 1.4)
- `cons.price.idx`: Consumer price index (monthly, 92.2 to 94.8)
- `cons.conf.idx`: Consumer confidence index (monthly, -50.8 to -26.9)
- `euribor3m`: Euribor 3-month rate (daily, 0.6 to 5.0)
- `nr.employed`: Number of employees (quarterly, 4963 to 5228)

### 2.4 Data Quality Issues

#### Unknown Values Analysis

| Feature | Unknown Count | Percentage | Strategy |
|---------|---------------|------------|----------|
| `job` | 330 | 0.80% | Impute mode |
| `marital` | 80 | 0.19% | Impute mode |
| `education` | 1,731 | 4.20% | Impute mode |
| `housing` | 990 | 2.40% | Impute mode |
| `loan` | 990 | 2.40% | Impute mode |
| `default` | 8,597 | 20.87% | Keep as category |

**Note**: The high percentage of unknowns in `default` (20.87%) suggests this information is genuinely unavailable rather than missing data, so it's kept as a separate category.

---

## 3. Data Preprocessing & Feature Engineering

### 3.1 Preprocessing Pipeline

The preprocessing pipeline consists of 4 sequential steps:

```
Raw Data â†’ Unknown Handler â†’ Feature Engineer â†’ Target Encoder â†’ Standard Scaler â†’ ML Model
```

#### Step 1: Unknown Values Handler
- **Purpose**: Handle "unknown" categorical values
- **Strategies**:
  - **Mode Imputation**: For features with <5% unknowns (job, marital, education, housing, loan)
  - **Keep as Category**: For default (20.87% unknowns - likely genuine unavailability)

#### Step 2: Feature Engineering
Created 6 new features to capture domain knowledge:

1. **age_group**: Categorical age bins (0-25, 26-35, 36-45, 46-60, 60+)
   - Rationale: Different age groups have different financial behaviors
   
2. **previously_contacted**: Binary flag (pdays != 999)
   - Rationale: Previous contact indicates engagement level
   
3. **contact_rate**: campaign / (previous + 1)
   - Rationale: Measures contact intensity relative to history
   
4. **total_contacts**: campaign + previous
   - Rationale: Total engagement across all campaigns
   
5. **has_any_loan**: Binary flag (housing=yes OR loan=yes)
   - Rationale: Overall financial commitment indicator
   
6. **economic_score**: Composite index combining:
   - emp.var.rate (normalized)
   - cons.conf.idx (normalized)
   - euribor3m (normalized)
   - Rationale: Economic conditions significantly impact financial decisions

**Final Feature Count**: 20 original + 6 engineered = **26 features** (after removing `duration`)

#### Step 3: Target Encoding
- **Method**: SafeTargetEncoder with minimum frequency threshold = 5
- **Applied to**: All categorical variables
- **Advantage**: Captures target relationship while preventing overfitting
- **Fit on**: Training set only (prevents data leakage)

#### Step 4: Feature Scaling
- **Method**: StandardScaler (zero mean, unit variance)
- **Applied to**: All numerical features
- **Rationale**: Required for Logistic Regression; improves convergence

### 3.2 Non-Realistic Feature Removal

**Removed Feature**: `duration` (last contact duration)

**Justification**: This feature has a strong correlation with the target (longer calls often lead to subscriptions), BUT it is not available before making the contact. Including it would cause **data leakage** and create an unrealistic model that cannot be deployed in production.

### 3.3 Class Imbalance Handling

**Method**: SMOTE (Synthetic Minority Over-sampling Technique)

- **Applied to**: Training set only (NOT test set)
- **Parameters**: k_neighbors=5, random_state=42
- **Results**:
  - Before SMOTE: {0: 29,238, 1: 3,712} (88.9% vs 11.1%)
  - After SMOTE: {0: 29,238, 1: 29,238} (50% vs 50%)

**Rationale**: Tree-based models and logistic regression can struggle with severe imbalance, leading to models that simply predict the majority class. SMOTE creates synthetic examples of the minority class to balance training data.

### 3.4 Data Transformation Summary

| Transformation | Justification | Impact |
|----------------|---------------|--------|
| **Remove duration** | Not available before contact (data leakage) | -1 feature |
| **Unknown imputation** | Recover information from missing categorical data | Improved data quality |
| **Feature engineering** | Capture domain knowledge and interactions | +6 features |
| **Target encoding** | Handle high-cardinality categoricals efficiently | Better than one-hot encoding |
| **SMOTE** | Address 8:1 class imbalance | Balanced training set |
| **Scaling** | Normalize feature ranges | Improved model convergence |

---

## 4. Machine Learning Models

### 4.1 Model Selection Rationale

Three diverse algorithms were selected to compare different learning paradigms:

#### 4.1.1 Logistic Regression
- **Type**: Linear model with logistic link function
- **Advantages**:
  - Fast training and prediction
  - Interpretable coefficients
  - Probabilistic outputs
  - Works well with linearly separable data
- **Hyperparameters Tuned**:
  - `C`: Inverse regularization strength [0.01, 0.1, 1, 10]
  - `penalty`: Regularization type ['l2']
  - `solver`: Optimization algorithm ['lbfgs', 'saga']

#### 4.1.2 Random Forest
- **Type**: Ensemble of decision trees (bagging)
- **Advantages**:
  - Handles non-linear relationships
  - Robust to outliers
  - Feature importance scores
  - No assumptions about data distribution
- **Hyperparameters Tuned**:
  - `n_estimators`: Number of trees [100, 200, 300]
  - `max_depth`: Tree depth [10, 20, 30, None]
  - `min_samples_split`: Minimum samples to split [2, 5, 10]
  - `min_samples_leaf`: Minimum samples per leaf [1, 2, 4]

#### 4.1.3 XGBoost
- **Type**: Gradient boosted decision trees
- **Advantages**:
  - State-of-the-art performance
  - Built-in regularization
  - Handles missing values
  - Efficient implementation
- **Hyperparameters Tuned**:
  - `n_estimators`: Number of boosting rounds [100, 200, 300]
  - `max_depth`: Tree depth [3, 5, 7]
  - `learning_rate`: Step size shrinkage [0.01, 0.1, 0.3]
  - `subsample`: Fraction of samples per tree [0.8, 0.9, 1.0]

### 4.2 Training Strategy

#### Data Splitting
```
Original Dataset (41,188 samples)
    â”‚
    â”œâ”€â”€ Training Set (32,950 - 80%) â†’ SMOTE â†’ (58,476 balanced)
    â”‚   â””â”€â”€ Used for: Model fitting & hyperparameter tuning
    â”‚
    â””â”€â”€ Test Set (8,238 - 20%)
        â””â”€â”€ Used for: Final unbiased evaluation
```

**Stratification**: Maintained class distribution in train/test split
- Training target: 88.9% No, 11.1% Yes
- Test target: 88.8% No, 11.2% Yes

#### Cross-Validation
- **Method**: Stratified K-Fold
- **Folds**: 5
- **Metric**: AUC-ROC (primary)
- **Purpose**: Hyperparameter selection and overfitting detection

### 4.3 Hyperparameter Optimization

**Method**: RandomizedSearchCV
- **Search Strategy**: Random sampling from parameter distributions
- **Iterations**: 20 per model
- **CV Folds**: 5
- **Scoring Metric**: AUC-ROC
- **Parallelization**: n_jobs=-1 (all CPU cores)

**Rationale for RandomizedSearch over GridSearch**:
- More efficient for large parameter spaces
- Better exploration of parameter combinations
- Faster convergence to good solutions

### 4.4 Selected Hyperparameters

#### Logistic Regression (Best Model)
```python
{
    'C': 0.1,           # Strong regularization
    'penalty': 'l2',     # Ridge regularization
    'solver': 'saga'     # Supports l1/l2 penalties
}
```

#### Random Forest
```python
{
    'n_estimators': 200,
    'max_depth': 20,
    'min_samples_split': 5,
    'min_samples_leaf': 2
}
```

#### XGBoost
```python
{
    'n_estimators': 200,
    'max_depth': 5,
    'learning_rate': 0.1,
    'subsample': 0.9
}
```

---

## 5. Results & Performance Analysis

### 5.1 Model Comparison Table

| Model | Test AUC-ROC | CV AUC-ROC | Precision | Recall | F1-Score | Accuracy |
|-------|--------------|------------|-----------|--------|----------|----------|
| **Logistic Regression*** | **0.7963** | 0.7938 Â± 0.0037 | 0.3458 | **0.6756** | 0.4575 | 0.8195 |
| XGBoost | 0.7685 | 0.9745 Â± 0.0486 | 0.3180 | 0.4688 | 0.3789 | 0.8269 |
| Random Forest | 0.7496 | 0.9854 Â± 0.0238 | 0.3336 | 0.4763 | 0.3924 | 0.8338 |

**Best Model**: Logistic Regression (highlighted with *)

### 5.2 Best Model: Logistic Regression Analysis

#### 5.2.1 Confusion Matrix

```
                  Predicted
                  NO      YES     Total
Actual  NO      6,124   1,186    7,310 (88.7%)
        YES       301     627      928 (11.3%)
        
        Total   6,425   1,813    8,238
```

**Confusion Matrix Interpretation**:
- **True Negatives (6,124)**: Correctly predicted no subscription - 83.8% of actual negatives
- **False Positives (1,186)**: Predicted subscription but didn't subscribe - 16.2% of actual negatives
- **False Negatives (301)**: Missed potential customers - 32.4% of actual positives
- **True Positives (627)**: Correctly identified subscribers - 67.6% of actual positives

#### 5.2.2 Detailed Classification Metrics

**Class 0 (No Subscription)**:
- Precision: 0.95 (95% of predicted "No" are correct)
- Recall: 0.84 (84% of actual "No" are identified)
- F1-Score: 0.89

**Class 1 (Yes Subscription)**:
- Precision: 0.35 (35% of predicted "Yes" are correct)
- Recall: 0.68 (68% of actual "Yes" are identified)
- F1-Score: 0.46

**Overall**:
- Accuracy: 82% (not the best metric due to imbalance)
- Macro Average F1: 0.67
- Weighted Average F1: 0.84

#### 5.2.3 ROC Curve Analysis

**AUC-ROC = 0.7963**

Interpretation:
- 79.63% probability that the model ranks a random positive instance higher than a random negative instance
- Significantly better than random classifier (AUC = 0.50)
- Good discrimination ability between classes
- Room for improvement but acceptable for production deployment

### 5.3 Cross-Validation Results Analysis

#### Logistic Regression (Best Model)
- **CV Mean**: 0.7938
- **CV Std Dev**: 0.0037
- **Test Score**: 0.7963
- **Generalization Gap**: +0.0025 (excellent!)

**Analysis**: Extremely stable performance across folds (std = 0.0037) and perfect generalization from CV to test. No signs of overfitting.

#### XGBoost
- **CV Mean**: 0.9745
- **CV Std Dev**: 0.0486
- **Test Score**: 0.7685
- **Generalization Gap**: -0.2060 (severe overfitting!)

**Analysis**: Excellent CV performance but massive drop on test set (-20.6%). This indicates severe overfitting despite regularization. High variance across folds (std = 0.0486) is a warning sign.

#### Random Forest
- **CV Mean**: 0.9854
- **CV Std Dev**: 0.0238
- **Test Score**: 0.7496
- **Generalization Gap**: -0.2358 (severe overfitting!)

**Analysis**: Best CV performance (98.5%) but worst test performance. The model memorized training patterns that don't generalize. The 23.6% drop is unacceptable for deployment.

### 5.4 Why Logistic Regression Outperformed Tree Models?

**Surprising Result**: The simpler linear model outperformed complex ensemble methods.

**Possible Explanations**:

1. **SMOTE + Tree Models = Overfitting**:
   - Tree models can memorize synthetic SMOTE samples
   - Linear models are more resistant to synthetic noise
   - SMOTE may create unrealistic decision boundaries that trees exploit

2. **High-Dimensional Data**:
   - 26 features after engineering
   - Tree models prone to overfitting in high dimensions
   - Logistic regression benefits from regularization

3. **Linear Separability**:
   - The problem may be more linearly separable than expected
   - Economic indicators and demographics may have linear relationships with target

4. **Proper Regularization**:
   - C=0.1 provides strong regularization
   - Tree models may need stronger constraints (not fully explored)

### 5.5 Feature Importance Analysis

**Top 10 Most Important Features** (from tree models):

| Rank | Feature | Importance | Category |
|------|---------|------------|----------|
| 1 | euribor3m | 0.185 | Economic |
| 2 | nr.employed | 0.142 | Economic |
| 3 | emp.var.rate | 0.098 | Economic |
| 4 | economic_score | 0.087 | Engineered |
| 5 | age | 0.076 | Demographic |
| 6 | pdays | 0.063 | Campaign |
| 7 | cons.price.idx | 0.052 | Economic |
| 8 | campaign | 0.049 | Campaign |
| 9 | previous | 0.042 | Campaign |
| 10 | contact_rate | 0.038 | Engineered |

**Key Insights**:
- **Economic indicators dominate** (top 3 features): Economic conditions are the strongest predictors
- **Engineered features add value**: economic_score (#4) and contact_rate (#10) appear in top 10
- **Campaign history matters**: pdays, campaign, and previous are all important
- **Demographics secondary**: Age is the only demographic in top 10

---

## 6. Business Impact & Deployment Strategy

### 6.1 Model Performance in Business Terms

#### Current Baseline (No Model)
- Contact all 8,238 clients
- Expected conversions: 928 (11.3%)
- Wasted contacts: 7,310 (88.7%)

#### With Logistic Regression Model (Threshold = 0.5)
- Contact only predicted positives: 1,813 (22% of total)
- True conversions captured: 627 (67.6% of all positives)
- **Cost reduction**: 78% fewer calls
- **Conversion rate**: 34.6% (vs 11.3% baseline) â†’ **3x improvement**
- **Missed opportunities**: 301 potential customers (32.4%)

#### Alternative Threshold Strategy (Threshold = 0.3 for higher recall)
By adjusting the classification threshold, we can trade precision for recall:
- Contact predicted positives: ~3,500 (42% of total)
- True conversions captured: ~800 (86% of all positives)
- **Cost reduction**: 58% fewer calls
- **Conversion rate**: 23% (2x improvement)
- **Missed opportunities**: Only 14% of potential customers

### 6.2 ROI Calculation

**Assumptions**:
- Cost per call: â‚¬5
- Revenue per conversion: â‚¬500
- Total budget without model: 8,238 Ã— â‚¬5 = â‚¬41,190
- Total revenue without model: 928 Ã— â‚¬500 = â‚¬464,000
- ROI without model: (464,000 - 41,190) / 41,190 = 1,026%

**With Model (threshold = 0.5)**:
- Campaign cost: 1,813 Ã— â‚¬5 = â‚¬9,065
- Revenue: 627 Ã— â‚¬500 = â‚¬313,500
- ROI: (313,500 - 9,065) / 9,065 = **3,357%** â†’ **+227% improvement**
- Net profit increase: â‚¬281,245 vs â‚¬422,810 (saves â‚¬32K in costs, loses â‚¬150K in revenue)

**Optimal Strategy**: Use threshold = 0.35 to balance cost savings with revenue capture.

### 6.3 Deployment Recommendations

1. **Model Monitoring**:
   - Track AUC-ROC weekly
   - Monitor precision/recall drift
   - Retrain quarterly with new data

2. **A/B Testing**:
   - Deploy to 20% of campaigns initially
   - Compare conversion rates vs control group
   - Gradually increase adoption if successful

3. **Threshold Optimization**:
   - Business should decide precision vs recall tradeoff
   - Consider different thresholds for different customer segments
   - Use cost-benefit analysis for threshold selection

4. **Feature Updates**:
   - Ensure economic indicators are updated daily
   - Handle new categorical values gracefully
   - Monitor feature drift over time

---

## 7. Conclusions

### 7.1 Key Findings

1. **Best Model Performance**:
   - Logistic Regression achieved AUC-ROC of 0.7963
   - 67.6% recall captures majority of potential customers
   - Excellent generalization (CV: 0.7938 vs Test: 0.7963)
   - Surprisingly outperformed complex ensemble methods

2. **Overfitting in Complex Models**:
   - Random Forest and XGBoost showed severe overfitting
   - CV scores (>0.97) drastically dropped on test set (~0.75)
   - SMOTE may have caused synthetic patterns that trees memorized
   - Simpler models generalize better for this problem

3. **Feature Importance**:
   - Economic indicators are the strongest predictors (euribor3m, nr.employed)
   - Engineered features added significant value
   - Campaign history crucial for prediction
   - Removing `duration` was necessary despite performance cost

4. **Class Imbalance Handling**:
   - SMOTE successfully balanced training data
   - Enabled models to learn minority class patterns
   - May have contributed to overfitting in tree models

### 7.2 Business Value

âœ… **Achieved Objectives**:
- Reduced contact costs by up to 78%
- Improved conversion rate from 11.3% to 34.6% (3x improvement)
- Identified 68% of potential customers with only 22% of contacts
- Deployable model with stable performance

### 7.3 Limitations & Future Work

#### Current Limitations:
1. **Moderate Precision**: 34.6% means 2 out of 3 predicted positives are false
2. **Missing Context**: Removed `duration` significantly impacts performance
3. **Temporal Aspects**: Model doesn't account for seasonal patterns explicitly
4. **Economic Dependency**: Heavy reliance on economic indicators may limit applicability

#### Future Improvements:
1. **Advanced Techniques**:
   - Try cost-sensitive learning instead of SMOTE
   - Experiment with neural networks for non-linear patterns
   - Ensemble of linear models (stacking)
   - Calibrate probability outputs for better threshold selection

2. **Feature Engineering**:
   - Create interaction terms (age Ã— economic_score)
   - Add temporal features (time since last campaign)
   - Incorporate external data (competitor rates, market trends)
   - Customer lifetime value predictions

3. **Model Refinement**:
   - Deeper hyperparameter search for tree models
   - Try different balancing techniques (class weights, undersampling)
   - Develop separate models for different customer segments
   - Multi-stage modeling (first filter, then rank)

4. **Deployment**:
   - Real-time prediction API
   - A/B testing framework
   - Automated retraining pipeline
   - Explainability module for business users

### 7.4 Final Remarks

This project successfully developed a production-ready classification model that delivers measurable business value. The Logistic Regression model, despite being the simplest approach, demonstrated superior generalization and stability compared to complex ensemble methods. This reinforces the principle that **model simplicity and proper regularization often outperform complexity**, especially when dealing with synthetic data augmentation.

The model enables the bank to optimize marketing resources while maintaining high customer reach, representing a significant advancement over the baseline random contact strategy. With proper monitoring and periodic retraining, this system can deliver sustained improvements in campaign efficiency and ROI.

---

## 8. References

1. [Moro et al., 2014] S. Moro, P. Cortez and P. Rita. A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems, Elsevier, 62:22-31, June 2014

2. UCI Machine Learning Repository - Bank Marketing Dataset: https://archive.ics.uci.edu/ml/datasets/bank+marketing

3. Chawla, N. V., et al. (2002). SMOTE: Synthetic Minority Over-sampling Technique. Journal of Artificial Intelligence Research, 16, 321-357.

4. Scikit-learn Documentation: https://scikit-learn.org/stable/

5. XGBoost Documentation: https://xgboost.readthedocs.io/

6. Imbalanced-learn Documentation: https://imbalanced-learn.org/

---

## Appendix A: Code Implementation

The complete implementation consists of three main modules:

1. **feature_engineer.py**: Custom transformers for unknown value handling, feature engineering, and safe target encoding

2. **pre_processing.py**: `BankMarketingProcessor` class handling data loading, EDA, preprocessing pipeline, and SMOTE application

3. **modeling.py**: `BankMarketingModeler` class implementing model training, hyperparameter optimization, cross-validation, and evaluation

**Key Design Principles**:
- Modular architecture with clear separation of concerns
- Pipeline-based preprocessing to prevent data leakage
- Comprehensive logging for reproducibility
- Configurable via JSON for easy experimentation

---

## Appendix B: Visualization Gallery

**Generated Visualizations**:
1. `roc_curves.png` - ROC curves comparing all three models
2. `confusion_matrices.png` - Confusion matrices for all models
3. `feature_importance_RandomForest.png` - Feature importance from Random Forest
4. `feature_importance_XGBoost.png` - Feature importance from XGBoost

All visualizations saved in `data/` directory.

---

**End of Report**



## Key Takeaways â€“ Bank Marketing ML Project
#  Modeling Insights
- Insight	ðŸ“ˆ Impact / Interpretation
Logistic Regression outperformed complex models	Achieved best AUC-ROC (0.7963) and strongest generalization; simple, regularized model resisted SMOTE overfitting.
Tree models (RF, XGBoost) overfit heavily	CV AUC > 0.97 but Test AUC < 0.77 â†’ memorized synthetic samples from SMOTE.
Economic indicators dominate feature importance	euribor3m, nr.employed, emp.var.rate explained over 40% of model variance.
Feature engineering boosted performance	New variables like economic_score and contact_rate improved recall and interpretability.
Removing â€œdurationâ€ avoided data leakage	Slight AUC drop but ensured deployability â€” realistic, ethical modeling choice.

#  Data & Methodology Highlights
-  Step	âœ… Key Decision
Missing Data Handling	Mode imputation for low-missing fields; retained â€œunknownâ€ for default (true missingness).
Balancing Technique	SMOTE applied only on training set to address 8:1 imbalance safely.
Encoding & Scaling	Target encoding for categorical features; StandardScaler for numeric ones.
Cross-Validation	Stratified 5-Fold CV (AUC metric) ensured stable and unbiased results.
Regularization	Logistic Regression with C=0.1 minimized overfitting; strong L2 penalty.

---
# Performance Summary
- Metric	Logistic Regression	Random Forest	XGBoost
- AUC-ROC	â­ 0.7963	0.7496	0.7685
- Recall (Positive Class)	67.6%	47.6%	46.8%
- Precision	34.6%	33.3%	31.8%
- Accuracy	81.9%	83.3%	82.7%
- Overfitting?	âŒ No	âš ï¸ Yes	âš ï¸ Yes

 Winner: Logistic Regression â€“ best trade-off between performance, generalization, and interpretability.

-  Business Impact
- Metric	Baseline (No Model)	With ML Model (Threshold=0.5)	Improvement
- Contacts Made	8,238	1,813	â–¼ -78%
- Conversions Captured	928	627	68% coverage
- Conversion Rate	11.3%	34.6%	3Ã— higher
- Cost Reduction	â€“	â‚¬32,125 saved	Major efficiency gain
- ROI	1,026%	3,357%	+227% increase

The model cuts costs by 78% while maintaining two-thirds of potential conversions â€” a major strategic win.
Next Steps
- Improvement Area	 Recommendation
Threshold Optimization	Tune cutoff to 0.3â€“0.4 range for higher recall (e.g., 86% capture rate).
Explainability	Add SHAP/LIME visualizations to justify decisions for regulators and managers.
Temporal Features	Incorporate seasonality (month trends, campaign cycles).
Cost-Sensitive Learning	Replace SMOTE with weighted loss to prevent synthetic overfitting.
Automated Retraining	Retrain quarterly; monitor AUC and drift metrics continuously.
---
 
Final Message
â€œSimplicity scales. Logistic Regression, when well-regularized and properly engineered, outperformed more complex ensembles â€” proving that interpretability and performance can coexist in business-critical applications.â€